{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 08. Correlation Analysis: News Sentiment and Stock Movement\n",
        "\n",
        "This notebook analyzes the correlation between news sentiment and stock price movements.\n",
        "\n",
        "**Sections:**\n",
        "1. **Data Loading and Preparation** - Load news and stock data, align dates\n",
        "2. **Sentiment Analysis** - Perform sentiment analysis on headlines using NLTK, TextBlob, and VADER\n",
        "3. **Stock Movement Calculation** - Calculate daily stock returns\n",
        "4. **Correlation Analysis** - Aggregate daily sentiments and calculate correlation with stock returns\n",
        "\n",
        "**Dependencies:** \n",
        "- Run `01_Data_Loading_and_Setup.ipynb` first for processed news data\n",
        "- Run `07_Quantitative_Analysis.ipynb` for stock price data (optional)\n",
        "- Stock price data files in `data/Data/Data/` directory\n",
        "\n",
        "**Stocks Analyzed:** AAPL, AMZN, GOOG, META, MSFT, NVDA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sys\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Sentiment Analysis libraries\n",
        "import nltk\n",
        "from textblob import TextBlob\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Statistical Analysis\n",
        "from scipy import stats\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Add utils to path\n",
        "sys.path.append('.')\n",
        "from utils import setup_plotting_style, load_processed_data\n",
        "\n",
        "# Setup plotting style\n",
        "setup_plotting_style()\n",
        "\n",
        "# Download required NLTK data\n",
        "print(\"Downloading NLTK data...\")\n",
        "try:\n",
        "    try:\n",
        "        nltk.download('punkt_tab', quiet=True)\n",
        "        print(\"Downloaded punkt_tab\")\n",
        "    except Exception as e1:\n",
        "        try:\n",
        "            nltk.download('punkt', quiet=True)\n",
        "            print(\"Downloaded punkt (fallback)\")\n",
        "        except Exception as e2:\n",
        "            print(f\"Warning: Could not download punkt: {e2}\")\n",
        "    \n",
        "    try:\n",
        "        nltk.download('stopwords', quiet=True)\n",
        "        print(\"Downloaded stopwords\")\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not download stopwords: {e}\")\n",
        "    \n",
        "    print(\"NLTK data ready!\")\n",
        "except Exception as e:\n",
        "    print(f\"NLTK download error: {e}\")\n",
        "\n",
        "# Initialize VADER sentiment analyzer\n",
        "vader_analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "print(\"\\nLibraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading and Preparation\n",
        "\n",
        "**Objective:** Load news and stock price data, then align dates to ensure each news item matches the corresponding stock trading day.\n",
        "\n",
        "**Key Steps:**\n",
        "- Load processed news data\n",
        "- Load stock price data for all stocks\n",
        "- Normalize timestamps and align dates\n",
        "- Handle missing dates and trading day matching\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load processed news data\n",
        "print(\"=\"*80)\n",
        "print(\"LOADING NEWS DATA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "try:\n",
        "    df_news = load_processed_data('../data/processed/df_processed.pkl')\n",
        "    print(f\"Loaded processed news data: {df_news.shape}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Processed data not found. Loading raw data...\")\n",
        "    from utils import load_data, preprocess_data, clean_data\n",
        "    df_news = load_data('../data/raw_analyst_ratings.csv')\n",
        "    df_news = clean_data(df_news, remove_duplicates=True, handle_missing='report', fix_dates=True)\n",
        "    df_news = preprocess_data(df_news)\n",
        "    print(f\"Loaded and preprocessed news data: {df_news.shape}\")\n",
        "\n",
        "print(f\"\\nNews dataset shape: {df_news.shape}\")\n",
        "print(f\"Columns: {list(df_news.columns)}\")\n",
        "\n",
        "# Ensure date is datetime\n",
        "if not pd.api.types.is_datetime64_any_dtype(df_news['date']):\n",
        "    df_news['date'] = pd.to_datetime(df_news['date'], errors='coerce')\n",
        "\n",
        "# Extract date only (without time) for alignment\n",
        "df_news['date_only'] = df_news['date'].dt.date\n",
        "\n",
        "# Filter out invalid dates\n",
        "df_news_valid = df_news[df_news['date'].notna()].copy()\n",
        "print(f\"\\nValid news dates: {len(df_news_valid):,} ({len(df_news_valid)/len(df_news)*100:.2f}%)\")\n",
        "print(f\"Date range: {df_news_valid['date'].min()} to {df_news_valid['date'].max()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load stock price data\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"LOADING STOCK PRICE DATA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "STOCKS = ['AAPL', 'AMZN', 'GOOG', 'META', 'MSFT', 'NVDA']\n",
        "DATA_PATH = '../data/Data/Data/'\n",
        "\n",
        "stock_data = {}\n",
        "\n",
        "for stock in STOCKS:\n",
        "    file_path = f\"{DATA_PATH}{stock}.csv\"\n",
        "    try:\n",
        "        df_stock = pd.read_csv(file_path)\n",
        "        \n",
        "        # Convert Date column to datetime\n",
        "        if 'Date' in df_stock.columns:\n",
        "            df_stock['Date'] = pd.to_datetime(df_stock['Date'])\n",
        "            df_stock = df_stock.sort_values('Date').reset_index(drop=True)\n",
        "            df_stock.set_index('Date', inplace=True)\n",
        "            df_stock.index.name = 'date'\n",
        "        \n",
        "        # Extract date only\n",
        "        df_stock['date_only'] = df_stock.index.date\n",
        "        \n",
        "        # Verify required columns\n",
        "        required_cols = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
        "        if all(col in df_stock.columns for col in required_cols):\n",
        "            stock_data[stock] = df_stock\n",
        "            print(f\"✓ {stock}: Loaded {len(df_stock):,} rows, Date range: {df_stock.index.min()} to {df_stock.index.max()}\")\n",
        "        else:\n",
        "            print(f\"⚠ {stock}: Missing required columns\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"✗ {stock}: File not found at {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"✗ {stock}: Error loading data - {e}\")\n",
        "\n",
        "print(f\"\\nSuccessfully loaded {len(stock_data)} stock datasets\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Date alignment: Normalize timestamps and match to trading days\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DATE ALIGNMENT\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# For news data: Convert to date only and ensure we're working with trading days\n",
        "# Stock markets are typically closed on weekends, so we need to match news to trading days\n",
        "\n",
        "# Get all trading days from stock data (union of all stock trading days)\n",
        "all_trading_days = set()\n",
        "for stock, df_stock in stock_data.items():\n",
        "    trading_days = set(df_stock.index.date)\n",
        "    all_trading_days.update(trading_days)\n",
        "\n",
        "all_trading_days = sorted(list(all_trading_days))\n",
        "print(f\"Total unique trading days across all stocks: {len(all_trading_days):,}\")\n",
        "print(f\"Trading day range: {all_trading_days[0]} to {all_trading_days[-1]}\")\n",
        "\n",
        "# Filter news data to only include dates that are trading days\n",
        "df_news_valid['is_trading_day'] = df_news_valid['date_only'].isin(all_trading_days)\n",
        "news_on_trading_days = df_news_valid[df_news_valid['is_trading_day']].copy()\n",
        "\n",
        "print(f\"\\nNews articles on trading days: {len(news_on_trading_days):,} ({len(news_on_trading_days)/len(df_news_valid)*100:.2f}%)\")\n",
        "print(f\"News articles on non-trading days: {(~df_news_valid['is_trading_day']).sum():,}\")\n",
        "\n",
        "# For correlation analysis, we'll use news on trading days\n",
        "df_news_aligned = news_on_trading_days.copy()\n",
        "print(f\"\\n✓ News data aligned to trading days: {len(df_news_aligned):,} articles\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Sentiment Analysis\n",
        "\n",
        "**Objective:** Perform sentiment analysis on headlines using multiple tools (NLTK, TextBlob, VADER) to quantify the tone of each article.\n",
        "\n",
        "**Tools Used:**\n",
        "- **TextBlob**: Polarity score (-1 to 1, where -1 is negative, 1 is positive)\n",
        "- **VADER**: Compound score (-1 to 1, optimized for social media and financial text)\n",
        "- **NLTK**: For text preprocessing\n",
        "\n",
        "**Key Questions:**\n",
        "- What is the distribution of sentiment scores?\n",
        "- How do different sentiment analyzers compare?\n",
        "- Are there differences in sentiment by stock or publisher?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform sentiment analysis on headlines\n",
        "print(\"=\"*80)\n",
        "print(\"SENTIMENT ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# For efficiency, we'll analyze a sample if the dataset is very large\n",
        "# You can adjust this or remove the sampling for full analysis\n",
        "SAMPLE_SIZE = None  # Set to None for full analysis, or a number like 100000 for sampling\n",
        "\n",
        "if SAMPLE_SIZE and len(df_news_aligned) > SAMPLE_SIZE:\n",
        "    print(f\"Sampling {SAMPLE_SIZE:,} articles for sentiment analysis...\")\n",
        "    df_sentiment = df_news_aligned.sample(n=SAMPLE_SIZE, random_state=42).copy()\n",
        "else:\n",
        "    df_sentiment = df_news_aligned.copy()\n",
        "\n",
        "print(f\"Analyzing sentiment for {len(df_sentiment):,} headlines...\")\n",
        "\n",
        "# Initialize sentiment analyzers\n",
        "vader_analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Calculate sentiment scores\n",
        "def calculate_textblob_sentiment(text):\n",
        "    \"\"\"Calculate TextBlob sentiment polarity\"\"\"\n",
        "    try:\n",
        "        blob = TextBlob(str(text))\n",
        "        return blob.sentiment.polarity\n",
        "    except:\n",
        "        return 0.0\n",
        "\n",
        "def calculate_vader_sentiment(text):\n",
        "    \"\"\"Calculate VADER sentiment compound score\"\"\"\n",
        "    try:\n",
        "        scores = vader_analyzer.polarity_scores(str(text))\n",
        "        return scores['compound']\n",
        "    except:\n",
        "        return 0.0\n",
        "\n",
        "# Apply sentiment analysis\n",
        "print(\"\\nCalculating TextBlob sentiment...\")\n",
        "df_sentiment['sentiment_textblob'] = df_sentiment['headline'].apply(calculate_textblob_sentiment)\n",
        "\n",
        "print(\"Calculating VADER sentiment...\")\n",
        "df_sentiment['sentiment_vader'] = df_sentiment['headline'].apply(calculate_vader_sentiment)\n",
        "\n",
        "# Create a combined sentiment score (average of both)\n",
        "df_sentiment['sentiment_combined'] = (df_sentiment['sentiment_textblob'] + df_sentiment['sentiment_vader']) / 2\n",
        "\n",
        "# Classify sentiment\n",
        "def classify_sentiment(score):\n",
        "    \"\"\"Classify sentiment as positive, negative, or neutral\"\"\"\n",
        "    if score > 0.1:\n",
        "        return 'positive'\n",
        "    elif score < -0.1:\n",
        "        return 'negative'\n",
        "    else:\n",
        "        return 'neutral'\n",
        "\n",
        "df_sentiment['sentiment_label'] = df_sentiment['sentiment_combined'].apply(classify_sentiment)\n",
        "\n",
        "print(f\"\\n✓ Sentiment analysis complete for {len(df_sentiment):,} headlines\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze sentiment distribution\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SENTIMENT DISTRIBUTION ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nSentiment Classification (Combined Score):\")\n",
        "sentiment_counts = df_sentiment['sentiment_label'].value_counts()\n",
        "for label, count in sentiment_counts.items():\n",
        "    pct = count / len(df_sentiment) * 100\n",
        "    print(f\"  {label:10s}: {count:>8,} ({pct:>5.2f}%)\")\n",
        "\n",
        "print(f\"\\nSentiment Score Statistics (Combined):\")\n",
        "print(f\"  Mean:   {df_sentiment['sentiment_combined'].mean():.4f}\")\n",
        "print(f\"  Median: {df_sentiment['sentiment_combined'].median():.4f}\")\n",
        "print(f\"  Std:    {df_sentiment['sentiment_combined'].std():.4f}\")\n",
        "print(f\"  Min:    {df_sentiment['sentiment_combined'].min():.4f}\")\n",
        "print(f\"  Max:    {df_sentiment['sentiment_combined'].max():.4f}\")\n",
        "\n",
        "print(f\"\\nSentiment Score Statistics (TextBlob):\")\n",
        "print(f\"  Mean:   {df_sentiment['sentiment_textblob'].mean():.4f}\")\n",
        "print(f\"  Median: {df_sentiment['sentiment_textblob'].median():.4f}\")\n",
        "\n",
        "print(f\"\\nSentiment Score Statistics (VADER):\")\n",
        "print(f\"  Mean:   {df_sentiment['sentiment_vader'].mean():.4f}\")\n",
        "print(f\"  Median: {df_sentiment['sentiment_vader'].median():.4f}\")\n",
        "\n",
        "# Sentiment by stock\n",
        "print(f\"\\n\" + \"=\"*80)\n",
        "print(\"SENTIMENT BY STOCK\")\n",
        "print(\"=\"*80)\n",
        "for stock in STOCKS:\n",
        "    stock_news = df_sentiment[df_sentiment['stock'] == stock]\n",
        "    if len(stock_news) > 0:\n",
        "        mean_sent = stock_news['sentiment_combined'].mean()\n",
        "        pos_pct = (stock_news['sentiment_label'] == 'positive').sum() / len(stock_news) * 100\n",
        "        neg_pct = (stock_news['sentiment_label'] == 'negative').sum() / len(stock_news) * 100\n",
        "        print(f\"  {stock:6s}: Mean sentiment = {mean_sent:>6.4f}, Positive = {pos_pct:>5.2f}%, Negative = {neg_pct:>5.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize sentiment distribution\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "fig.suptitle('Sentiment Analysis Results', fontsize=16, fontweight='bold', y=0.995)\n",
        "\n",
        "# Plot 1: Sentiment classification distribution\n",
        "ax1 = axes[0, 0]\n",
        "sentiment_counts = df_sentiment['sentiment_label'].value_counts()\n",
        "colors = {'positive': 'green', 'negative': 'red', 'neutral': 'gray'}\n",
        "bars = ax1.bar(sentiment_counts.index, sentiment_counts.values, \n",
        "               color=[colors.get(x, 'blue') for x in sentiment_counts.index])\n",
        "ax1.set_xlabel('Sentiment Label', fontsize=11)\n",
        "ax1.set_ylabel('Number of Articles', fontsize=11)\n",
        "ax1.set_title('Sentiment Classification Distribution', fontsize=12, fontweight='bold')\n",
        "ax1.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add value labels\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
        "             f'{int(height):,}',\n",
        "             ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "# Plot 2: Combined sentiment score distribution\n",
        "ax2 = axes[0, 1]\n",
        "ax2.hist(df_sentiment['sentiment_combined'], bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n",
        "ax2.axvline(df_sentiment['sentiment_combined'].mean(), color='r', linestyle='--', linewidth=2, \n",
        "            label=f'Mean ({df_sentiment[\"sentiment_combined\"].mean():.3f})')\n",
        "ax2.axvline(0, color='black', linestyle='-', linewidth=1, alpha=0.5)\n",
        "ax2.set_xlabel('Sentiment Score (Combined)', fontsize=11)\n",
        "ax2.set_ylabel('Frequency', fontsize=11)\n",
        "ax2.set_title('Combined Sentiment Score Distribution', fontsize=12, fontweight='bold')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Plot 3: Comparison of TextBlob vs VADER\n",
        "ax3 = axes[1, 0]\n",
        "# Sample for scatter plot if too many points\n",
        "if len(df_sentiment) > 10000:\n",
        "    sample_df = df_sentiment.sample(n=10000, random_state=42)\n",
        "else:\n",
        "    sample_df = df_sentiment\n",
        "ax3.scatter(sample_df['sentiment_textblob'], sample_df['sentiment_vader'], \n",
        "           alpha=0.3, s=10, color='purple')\n",
        "ax3.plot([-1, 1], [-1, 1], 'r--', linewidth=1, alpha=0.5, label='Perfect Agreement')\n",
        "ax3.set_xlabel('TextBlob Sentiment', fontsize=11)\n",
        "ax3.set_ylabel('VADER Sentiment', fontsize=11)\n",
        "ax3.set_title('TextBlob vs VADER Sentiment Comparison', fontsize=12, fontweight='bold')\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 4: Sentiment by stock\n",
        "ax4 = axes[1, 1]\n",
        "sentiment_by_stock = df_sentiment.groupby('stock')['sentiment_combined'].mean().sort_values()\n",
        "bars = ax4.bar(range(len(sentiment_by_stock)), sentiment_by_stock.values, color='coral')\n",
        "ax4.set_xticks(range(len(sentiment_by_stock)))\n",
        "ax4.set_xticklabels(sentiment_by_stock.index, rotation=45, ha='right')\n",
        "ax4.set_ylabel('Mean Sentiment Score', fontsize=11)\n",
        "ax4.set_title('Average Sentiment by Stock', fontsize=12, fontweight='bold')\n",
        "ax4.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
        "ax4.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add value labels\n",
        "for i, (stock, val) in enumerate(sentiment_by_stock.items()):\n",
        "    ax4.text(i, val, f'{val:.3f}', ha='center', va='bottom' if val > 0 else 'top', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('figures/sentiment_analysis.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"Saved: figures/sentiment_analysis.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Stock Movement Calculation\n",
        "\n",
        "**Objective:** Calculate daily stock returns to represent stock movements.\n",
        "\n",
        "**Key Steps:**\n",
        "- Calculate daily percentage change in closing prices\n",
        "- Handle missing trading days\n",
        "- Analyze return distribution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate daily stock returns\n",
        "print(\"=\"*80)\n",
        "print(\"STOCK MOVEMENT CALCULATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "stock_returns = {}\n",
        "\n",
        "for stock, df_stock in stock_data.items():\n",
        "    # Calculate daily returns (percentage change)\n",
        "    df_stock['daily_return'] = df_stock['Close'].pct_change() * 100  # Convert to percentage\n",
        "    \n",
        "    # Create returns dataframe with date\n",
        "    returns_df = pd.DataFrame({\n",
        "        'date': df_stock.index,\n",
        "        'date_only': df_stock.index.date,\n",
        "        'daily_return': df_stock['daily_return'],\n",
        "        'close_price': df_stock['Close']\n",
        "    })\n",
        "    \n",
        "    stock_returns[stock] = returns_df\n",
        "    \n",
        "    # Statistics\n",
        "    returns_clean = returns_df['daily_return'].dropna()\n",
        "    print(f\"\\n{stock}:\")\n",
        "    print(f\"  Total trading days: {len(returns_df):,}\")\n",
        "    print(f\"  Mean daily return: {returns_clean.mean():.4f}%\")\n",
        "    print(f\"  Std dev: {returns_clean.std():.4f}%\")\n",
        "    print(f\"  Min return: {returns_clean.min():.4f}%\")\n",
        "    print(f\"  Max return: {returns_clean.max():.4f}%\")\n",
        "    print(f\"  Positive days: {(returns_clean > 0).sum():,} ({(returns_clean > 0).sum()/len(returns_clean)*100:.2f}%)\")\n",
        "    print(f\"  Negative days: {(returns_clean < 0).sum():,} ({(returns_clean < 0).sum()/len(returns_clean)*100:.2f}%)\")\n",
        "\n",
        "print(f\"\\n✓ Daily returns calculated for {len(stock_returns)} stocks\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize stock returns distribution\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "fig.suptitle('Daily Stock Returns Distribution', fontsize=16, fontweight='bold', y=0.995)\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, (stock, returns_df) in enumerate(stock_returns.items()):\n",
        "    ax = axes[idx]\n",
        "    returns_clean = returns_df['daily_return'].dropna()\n",
        "    \n",
        "    ax.hist(returns_clean, bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n",
        "    ax.axvline(returns_clean.mean(), color='r', linestyle='--', linewidth=2, \n",
        "              label=f'Mean ({returns_clean.mean():.3f}%)')\n",
        "    ax.axvline(0, color='black', linestyle='-', linewidth=1, alpha=0.5)\n",
        "    ax.set_xlabel('Daily Return (%)', fontsize=10)\n",
        "    ax.set_ylabel('Frequency', fontsize=10)\n",
        "    ax.set_title(f'{stock} Daily Returns', fontsize=11, fontweight='bold')\n",
        "    ax.legend(fontsize=8)\n",
        "    ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('figures/stock_returns_distribution.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"Saved: figures/stock_returns_distribution.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Correlation Analysis\n",
        "\n",
        "**Objective:** Aggregate daily sentiments and calculate correlation between news sentiment and stock returns.\n",
        "\n",
        "**Key Steps:**\n",
        "- Aggregate daily sentiment scores (average if multiple articles per day)\n",
        "- Match sentiment scores to stock returns by date and stock\n",
        "- Calculate Pearson correlation coefficient\n",
        "- Perform statistical significance testing\n",
        "- Analyze correlation patterns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aggregate daily sentiments\n",
        "print(\"=\"*80)\n",
        "print(\"AGGREGATING DAILY SENTIMENTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Group by date and stock, calculate average sentiment\n",
        "daily_sentiments = df_sentiment.groupby(['date_only', 'stock']).agg({\n",
        "    'sentiment_combined': 'mean',\n",
        "    'sentiment_textblob': 'mean',\n",
        "    'sentiment_vader': 'mean',\n",
        "    'headline': 'count'\n",
        "}).reset_index()\n",
        "\n",
        "daily_sentiments.columns = ['date_only', 'stock', 'avg_sentiment_combined', \n",
        "                           'avg_sentiment_textblob', 'avg_sentiment_vader', 'article_count']\n",
        "\n",
        "print(f\"\\nDaily sentiment aggregation:\")\n",
        "print(f\"  Total date-stock combinations: {len(daily_sentiments):,}\")\n",
        "print(f\"  Days with multiple articles: {(daily_sentiments['article_count'] > 1).sum():,}\")\n",
        "print(f\"  Average articles per day-stock: {daily_sentiments['article_count'].mean():.2f}\")\n",
        "print(f\"  Max articles in a single day-stock: {daily_sentiments['article_count'].max():,}\")\n",
        "\n",
        "# Show examples\n",
        "print(f\"\\nSample of daily aggregated sentiments:\")\n",
        "print(daily_sentiments.head(10).to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Merge sentiment and stock returns data\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MERGING SENTIMENT AND STOCK RETURNS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "correlation_data = {}\n",
        "\n",
        "for stock in STOCKS:\n",
        "    # Get sentiment data for this stock\n",
        "    stock_sentiment = daily_sentiments[daily_sentiments['stock'] == stock].copy()\n",
        "    \n",
        "    # Get returns data for this stock\n",
        "    stock_return = stock_returns[stock].copy()\n",
        "    \n",
        "    # Merge on date_only\n",
        "    merged = pd.merge(\n",
        "        stock_sentiment,\n",
        "        stock_return[['date_only', 'daily_return']],\n",
        "        on='date_only',\n",
        "        how='inner'\n",
        "    )\n",
        "    \n",
        "    # Remove rows with missing returns or sentiment\n",
        "    merged = merged.dropna(subset=['daily_return', 'avg_sentiment_combined'])\n",
        "    \n",
        "    correlation_data[stock] = merged\n",
        "    \n",
        "    print(f\"\\n{stock}:\")\n",
        "    print(f\"  Matched date-stock pairs: {len(merged):,}\")\n",
        "    print(f\"  Date range: {merged['date_only'].min()} to {merged['date_only'].max()}\")\n",
        "    print(f\"  Coverage: {len(merged)/len(stock_return)*100:.2f}% of trading days have news\")\n",
        "\n",
        "print(f\"\\n✓ Data merged for {len(correlation_data)} stocks\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate correlation coefficients\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CORRELATION ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "correlation_results = {}\n",
        "\n",
        "for stock, merged_df in correlation_data.items():\n",
        "    if len(merged_df) < 2:\n",
        "        print(f\"\\n{stock}: Insufficient data for correlation (need at least 2 data points)\")\n",
        "        continue\n",
        "    \n",
        "    # Calculate Pearson correlation\n",
        "    sentiment = merged_df['avg_sentiment_combined']\n",
        "    returns = merged_df['daily_return']\n",
        "    \n",
        "    # Remove any remaining NaN values\n",
        "    valid_mask = ~(sentiment.isna() | returns.isna())\n",
        "    sentiment_clean = sentiment[valid_mask]\n",
        "    returns_clean = returns[valid_mask]\n",
        "    \n",
        "    if len(sentiment_clean) < 2:\n",
        "        print(f\"\\n{stock}: Insufficient valid data for correlation\")\n",
        "        continue\n",
        "    \n",
        "    # Pearson correlation\n",
        "    corr_coef, p_value = pearsonr(sentiment_clean, returns_clean)\n",
        "    \n",
        "    # Calculate R-squared\n",
        "    r_squared = corr_coef ** 2\n",
        "    \n",
        "    correlation_results[stock] = {\n",
        "        'correlation': corr_coef,\n",
        "        'p_value': p_value,\n",
        "        'r_squared': r_squared,\n",
        "        'n_observations': len(sentiment_clean),\n",
        "        'significant': p_value < 0.05\n",
        "    }\n",
        "    \n",
        "    print(f\"\\n{stock}:\")\n",
        "    print(f\"  Pearson Correlation: {corr_coef:>7.4f}\")\n",
        "    print(f\"  R-squared:           {r_squared:>7.4f}\")\n",
        "    print(f\"  P-value:             {p_value:>7.4e}\")\n",
        "    print(f\"  Observations:        {len(sentiment_clean):>7,}\")\n",
        "    print(f\"  Significant (p<0.05): {'Yes' if p_value < 0.05 else 'No'}\")\n",
        "\n",
        "# Summary\n",
        "print(f\"\\n\" + \"=\"*80)\n",
        "print(\"CORRELATION SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "if correlation_results:\n",
        "    avg_corr = np.mean([r['correlation'] for r in correlation_results.values()])\n",
        "    significant_count = sum(1 for r in correlation_results.values() if r['significant'])\n",
        "    print(f\"Average correlation across stocks: {avg_corr:.4f}\")\n",
        "    print(f\"Stocks with significant correlation (p<0.05): {significant_count}/{len(correlation_results)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize correlations\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "fig.suptitle('Correlation Analysis: News Sentiment vs Stock Returns', fontsize=16, fontweight='bold', y=0.995)\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, stock in enumerate(STOCKS):\n",
        "    ax = axes[idx]\n",
        "    \n",
        "    if stock in correlation_data and len(correlation_data[stock]) > 0:\n",
        "        merged_df = correlation_data[stock]\n",
        "        \n",
        "        # Scatter plot\n",
        "        ax.scatter(merged_df['avg_sentiment_combined'], merged_df['daily_return'], \n",
        "                  alpha=0.5, s=20, color='steelblue')\n",
        "        \n",
        "        # Add trend line\n",
        "        z = np.polyfit(merged_df['avg_sentiment_combined'], merged_df['daily_return'], 1)\n",
        "        p = np.poly1d(z)\n",
        "        ax.plot(merged_df['avg_sentiment_combined'], p(merged_df['avg_sentiment_combined']), \n",
        "               \"r--\", alpha=0.8, linewidth=2, label=f\"Trend line\")\n",
        "        \n",
        "        # Add correlation info\n",
        "        if stock in correlation_results:\n",
        "            corr = correlation_results[stock]['correlation']\n",
        "            p_val = correlation_results[stock]['p_value']\n",
        "            sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else ''\n",
        "            ax.text(0.05, 0.95, f'r = {corr:.3f}{sig}\\np = {p_val:.3e}', \n",
        "                   transform=ax.transAxes, fontsize=10,\n",
        "                   verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "        \n",
        "        ax.set_xlabel('Average Daily Sentiment Score', fontsize=10)\n",
        "        ax.set_ylabel('Daily Return (%)', fontsize=10)\n",
        "        ax.set_title(f'{stock} - Sentiment vs Returns', fontsize=11, fontweight='bold')\n",
        "        ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5, alpha=0.3)\n",
        "        ax.axvline(x=0, color='black', linestyle='-', linewidth=0.5, alpha=0.3)\n",
        "        ax.legend(fontsize=8)\n",
        "        ax.grid(True, alpha=0.3)\n",
        "    else:\n",
        "        ax.text(0.5, 0.5, 'Insufficient data', ha='center', va='center', \n",
        "               transform=ax.transAxes, fontsize=12)\n",
        "        ax.set_title(f'{stock}', fontsize=11, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('figures/sentiment_returns_correlation.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"Saved: figures/sentiment_returns_correlation.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create correlation summary visualization\n",
        "if correlation_results:\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "    fig.suptitle('Correlation Analysis Summary', fontsize=16, fontweight='bold', y=1.0)\n",
        "    \n",
        "    # Plot 1: Correlation coefficients by stock\n",
        "    ax1 = axes[0]\n",
        "    stocks = list(correlation_results.keys())\n",
        "    correlations = [correlation_results[s]['correlation'] for s in stocks]\n",
        "    p_values = [correlation_results[s]['p_value'] for s in stocks]\n",
        "    colors = ['green' if p < 0.05 else 'red' for p in p_values]\n",
        "    \n",
        "    bars = ax1.bar(range(len(stocks)), correlations, color=colors, alpha=0.7)\n",
        "    ax1.set_xticks(range(len(stocks)))\n",
        "    ax1.set_xticklabels(stocks)\n",
        "    ax1.set_ylabel('Pearson Correlation Coefficient', fontsize=11)\n",
        "    ax1.set_title('Correlation: Sentiment vs Stock Returns by Stock', fontsize=12, fontweight='bold')\n",
        "    ax1.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
        "    ax1.grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    # Add value labels\n",
        "    for i, (corr, p_val) in enumerate(zip(correlations, p_values)):\n",
        "        sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else ''\n",
        "        ax1.text(i, corr, f'{corr:.3f}{sig}', ha='center', \n",
        "                va='bottom' if corr > 0 else 'top', fontsize=9)\n",
        "    \n",
        "    # Add legend\n",
        "    from matplotlib.patches import Patch\n",
        "    legend_elements = [\n",
        "        Patch(facecolor='green', alpha=0.7, label='Significant (p<0.05)'),\n",
        "        Patch(facecolor='red', alpha=0.7, label='Not Significant')\n",
        "    ]\n",
        "    ax1.legend(handles=legend_elements, loc='best', fontsize=9)\n",
        "    \n",
        "    # Plot 2: R-squared values\n",
        "    ax2 = axes[1]\n",
        "    r_squared = [correlation_results[s]['r_squared'] for s in stocks]\n",
        "    bars2 = ax2.bar(range(len(stocks)), r_squared, color='steelblue', alpha=0.7)\n",
        "    ax2.set_xticks(range(len(stocks)))\n",
        "    ax2.set_xticklabels(stocks)\n",
        "    ax2.set_ylabel('R-squared', fontsize=11)\n",
        "    ax2.set_title('R-squared: Variance Explained', fontsize=12, fontweight='bold')\n",
        "    ax2.grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    # Add value labels\n",
        "    for i, r2 in enumerate(r_squared):\n",
        "        ax2.text(i, r2, f'{r2:.3f}', ha='center', va='bottom', fontsize=9)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('figures/correlation_summary.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(\"Saved: figures/correlation_summary.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Additional analysis: Lagged correlation (sentiment today vs returns tomorrow)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"LAGGED CORRELATION ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "print(\"Analyzing correlation between sentiment and next-day returns...\")\n",
        "\n",
        "lagged_correlation_results = {}\n",
        "\n",
        "for stock in STOCKS:\n",
        "    if stock not in correlation_data or len(correlation_data[stock]) < 2:\n",
        "        continue\n",
        "    \n",
        "    merged_df = correlation_data[stock].copy()\n",
        "    merged_df = merged_df.sort_values('date_only').reset_index(drop=True)\n",
        "    \n",
        "    # Shift returns by 1 day (sentiment today -> returns tomorrow)\n",
        "    merged_df['next_day_return'] = merged_df['daily_return'].shift(-1)\n",
        "    \n",
        "    # Remove NaN from shifted column\n",
        "    lagged_df = merged_df[['avg_sentiment_combined', 'next_day_return']].dropna()\n",
        "    \n",
        "    if len(lagged_df) < 2:\n",
        "        continue\n",
        "    \n",
        "    corr_lag, p_val_lag = pearsonr(lagged_df['avg_sentiment_combined'], lagged_df['next_day_return'])\n",
        "    \n",
        "    lagged_correlation_results[stock] = {\n",
        "        'correlation': corr_lag,\n",
        "        'p_value': p_val_lag,\n",
        "        'n_observations': len(lagged_df)\n",
        "    }\n",
        "    \n",
        "    print(f\"\\n{stock}:\")\n",
        "    print(f\"  Lagged Correlation (sentiment -> next day return): {corr_lag:>7.4f}\")\n",
        "    print(f\"  P-value: {p_val_lag:>7.4e}\")\n",
        "    print(f\"  Significant (p<0.05): {'Yes' if p_val_lag < 0.05 else 'No'}\")\n",
        "\n",
        "if lagged_correlation_results:\n",
        "    print(f\"\\n\" + \"=\"*80)\n",
        "    print(\"LAGGED CORRELATION SUMMARY\")\n",
        "    print(\"=\"*80)\n",
        "    avg_lag_corr = np.mean([r['correlation'] for r in lagged_correlation_results.values()])\n",
        "    sig_lag_count = sum(1 for r in lagged_correlation_results.values() if r['p_value'] < 0.05)\n",
        "    print(f\"Average lagged correlation: {avg_lag_corr:.4f}\")\n",
        "    print(f\"Stocks with significant lagged correlation: {sig_lag_count}/{len(lagged_correlation_results)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Key Findings\n",
        "\n",
        "**Key Insights from Correlation Analysis:**\n",
        "\n",
        "1. **Sentiment Analysis Results:**\n",
        "   - [To be filled after running the notebook]\n",
        "\n",
        "2. **Stock Returns:**\n",
        "   - [To be filled after running the notebook]\n",
        "\n",
        "3. **Correlation Findings:**\n",
        "   - [To be filled after running the notebook]\n",
        "\n",
        "4. **Statistical Significance:**\n",
        "   - [To be filled after running the notebook]\n",
        "\n",
        "5. **Lagged Correlation:**\n",
        "   - [To be filled after running the notebook]\n",
        "\n",
        "**Next Steps:**\n",
        "- Interpret correlation results in business context\n",
        "- Consider additional factors (volume, volatility, market conditions)\n",
        "- Explore non-linear relationships if linear correlation is weak\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure figures directory exists\n",
        "figures_dir = 'figures'\n",
        "if not os.path.exists(figures_dir):\n",
        "    os.makedirs(figures_dir)\n",
        "    print(f\"Created {figures_dir}/ directory\")\n",
        "else:\n",
        "    print(f\"Using existing {figures_dir}/ directory\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
